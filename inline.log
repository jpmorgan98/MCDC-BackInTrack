.visible .func  (.param .b32 func_retval0) _ZN8__main__11complicatedB2v1B96cw51cXTLSUwHBinCqbbgUAAGBlq82ILSCEQYkgSQBFCjFSaBZJtttTo4sahbKUBDUB3kNVDaQRKChQ_2bSEFA_2fkGdcqwkAE6RecordILi439EE(
.param .b64 _ZN8__main__11complicatedB2v1B96cw51cXTLSUwHBinCqbbgUAAGBlq82ILSCEQYkgSQBFCjFSaBZJtttTo4sahbKUBDUB3kNVDaQRKChQ_2bSEFA_2fkGdcqwkAE6RecordILi439EE_param_0,
.param .b64 _ZN8__main__11complicatedB2v1B96cw51cXTLSUwHBinCqbbgUAAGBlq82ILSCEQYkgSQBFCjFSaBZJtttTo4sahbKUBDUB3kNVDaQRKChQ_2bSEFA_2fkGdcqwkAE6RecordILi439EE_param_1
)
{
.reg .pred 	%p<2>;
.reg .b16 	%rs<3>;
.reg .b32 	%r<2>;
.reg .f64 	%fd<7>;
.reg .b64 	%rd<118>;
ld.param.u64 	%rd1, [_ZN8__main__11complicatedB2v1B96cw51cXTLSUwHBinCqbbgUAAGBlq82ILSCEQYkgSQBFCjFSaBZJtttTo4sahbKUBDUB3kNVDaQRKChQ_2bSEFA_2fkGdcqwkAE6RecordILi439EE_param_0];
ld.param.u64 	%rd2, [_ZN8__main__11complicatedB2v1B96cw51cXTLSUwHBinCqbbgUAAGBlq82ILSCEQYkgSQBFCjFSaBZJtttTo4sahbKUBDUB3kNVDaQRKChQ_2bSEFA_2fkGdcqwkAE6RecordILi439EE_param_1];
ld.u8 	%rd3, [%rd2];
ld.u8 	%rd4, [%rd2+1];
bfi.b64 	%rd5, %rd4, %rd3, 8, 8;
ld.u8 	%rd6, [%rd2+2];
ld.u8 	%rd7, [%rd2+3];
bfi.b64 	%rd8, %rd7, %rd6, 8, 8;
bfi.b64 	%rd9, %rd8, %rd5, 16, 16;
ld.u8 	%rd10, [%rd2+4];
ld.u8 	%rd11, [%rd2+5];
bfi.b64 	%rd12, %rd11, %rd10, 8, 8;
ld.u8 	%rd13, [%rd2+6];
ld.u8 	%rd14, [%rd2+7];
bfi.b64 	%rd15, %rd14, %rd13, 8, 8;
bfi.b64 	%rd16, %rd15, %rd12, 16, 16;
bfi.b64 	%rd17, %rd16, %rd9, 32, 32;
mov.b64 	%fd1, %rd17;
add.f64 	%fd2, %fd1, 0d3FF0000000000000;
mov.b64 	%rd18, %fd2;
st.u8 	[%rd2], %rd18;
shr.u64 	%rd19, %rd18, 56;
st.u8 	[%rd2+7], %rd19;
shr.u64 	%rd20, %rd18, 48;
st.u8 	[%rd2+6], %rd20;
shr.u64 	%rd21, %rd18, 40;
st.u8 	[%rd2+5], %rd21;
shr.u64 	%rd22, %rd18, 32;
st.u8 	[%rd2+4], %rd22;
shr.u64 	%rd23, %rd18, 24;
st.u8 	[%rd2+3], %rd23;
shr.u64 	%rd24, %rd18, 16;
st.u8 	[%rd2+2], %rd24;
shr.u64 	%rd25, %rd18, 8;
st.u8 	[%rd2+1], %rd25;
ld.u8 	%rd26, [%rd2+8];
ld.u8 	%rd27, [%rd2+9];
bfi.b64 	%rd28, %rd27, %rd26, 8, 8;
ld.u8 	%rd29, [%rd2+10];
ld.u8 	%rd30, [%rd2+11];
bfi.b64 	%rd31, %rd30, %rd29, 8, 8;
bfi.b64 	%rd32, %rd31, %rd28, 16, 16;
ld.u8 	%rd33, [%rd2+12];
ld.u8 	%rd34, [%rd2+13];
bfi.b64 	%rd35, %rd34, %rd33, 8, 8;
ld.u8 	%rd36, [%rd2+14];
ld.u8 	%rd37, [%rd2+15];
bfi.b64 	%rd38, %rd37, %rd36, 8, 8;
bfi.b64 	%rd39, %rd38, %rd35, 16, 16;
bfi.b64 	%rd40, %rd39, %rd32, 32, 32;
mov.b64 	%fd3, %rd40;
add.f64 	%fd4, %fd3, 0d3FF0000000000000;
mov.b64 	%rd41, %fd4;
st.u8 	[%rd2+8], %rd41;
shr.u64 	%rd42, %rd41, 56;
st.u8 	[%rd2+15], %rd42;
shr.u64 	%rd43, %rd41, 48;
st.u8 	[%rd2+14], %rd43;
shr.u64 	%rd44, %rd41, 40;
st.u8 	[%rd2+13], %rd44;
shr.u64 	%rd45, %rd41, 32;
st.u8 	[%rd2+12], %rd45;
shr.u64 	%rd46, %rd41, 24;
st.u8 	[%rd2+11], %rd46;
shr.u64 	%rd47, %rd41, 16;
st.u8 	[%rd2+10], %rd47;
shr.u64 	%rd48, %rd41, 8;
st.u8 	[%rd2+9], %rd48;
ld.u8 	%rd49, [%rd2+16];
ld.u8 	%rd50, [%rd2+17];
bfi.b64 	%rd51, %rd50, %rd49, 8, 8;
ld.u8 	%rd52, [%rd2+18];
ld.u8 	%rd53, [%rd2+19];
bfi.b64 	%rd54, %rd53, %rd52, 8, 8;
bfi.b64 	%rd55, %rd54, %rd51, 16, 16;
ld.u8 	%rd56, [%rd2+20];
ld.u8 	%rd57, [%rd2+21];
bfi.b64 	%rd58, %rd57, %rd56, 8, 8;
ld.u8 	%rd59, [%rd2+22];
ld.u8 	%rd60, [%rd2+23];
bfi.b64 	%rd61, %rd60, %rd59, 8, 8;
bfi.b64 	%rd62, %rd61, %rd58, 16, 16;
bfi.b64 	%rd63, %rd62, %rd55, 32, 32;
mov.b64 	%fd5, %rd63;
add.f64 	%fd6, %fd5, 0d3FF0000000000000;
mov.b64 	%rd64, %fd6;
st.u8 	[%rd2+16], %rd64;
shr.u64 	%rd65, %rd64, 56;
st.u8 	[%rd2+23], %rd65;
shr.u64 	%rd66, %rd64, 48;
st.u8 	[%rd2+22], %rd66;
shr.u64 	%rd67, %rd64, 40;
st.u8 	[%rd2+21], %rd67;
shr.u64 	%rd68, %rd64, 32;
st.u8 	[%rd2+20], %rd68;
shr.u64 	%rd69, %rd64, 24;
st.u8 	[%rd2+19], %rd69;
shr.u64 	%rd70, %rd64, 16;
st.u8 	[%rd2+18], %rd70;
shr.u64 	%rd71, %rd64, 8;
st.u8 	[%rd2+17], %rd71;
ld.u8 	%rd72, [%rd2+24];
ld.u8 	%rd73, [%rd2+25];
bfi.b64 	%rd74, %rd73, %rd72, 8, 8;
ld.u8 	%rd75, [%rd2+26];
ld.u8 	%rd76, [%rd2+27];
bfi.b64 	%rd77, %rd76, %rd75, 8, 8;
bfi.b64 	%rd78, %rd77, %rd74, 16, 16;
ld.u8 	%rd79, [%rd2+28];
ld.u8 	%rd80, [%rd2+29];
bfi.b64 	%rd81, %rd80, %rd79, 8, 8;
ld.u8 	%rd82, [%rd2+30];
ld.u8 	%rd83, [%rd2+31];
bfi.b64 	%rd84, %rd83, %rd82, 8, 8;
bfi.b64 	%rd85, %rd84, %rd81, 16, 16;
bfi.b64 	%rd86, %rd85, %rd78, 32, 32;
add.s64 	%rd87, %rd86, 1;
st.u8 	[%rd2+24], %rd87;
shr.u64 	%rd88, %rd87, 56;
st.u8 	[%rd2+31], %rd88;
shr.u64 	%rd89, %rd87, 48;
st.u8 	[%rd2+30], %rd89;
shr.u64 	%rd90, %rd87, 40;
st.u8 	[%rd2+29], %rd90;
shr.u64 	%rd91, %rd87, 32;
st.u8 	[%rd2+28], %rd91;
shr.u64 	%rd92, %rd87, 24;
st.u8 	[%rd2+27], %rd92;
shr.u64 	%rd93, %rd87, 16;
st.u8 	[%rd2+26], %rd93;
shr.u64 	%rd94, %rd87, 8;
st.u8 	[%rd2+25], %rd94;
ld.u8 	%rd95, [%rd2+32];
ld.u8 	%rd96, [%rd2+33];
bfi.b64 	%rd97, %rd96, %rd95, 8, 8;
ld.u8 	%rd98, [%rd2+34];
ld.u8 	%rd99, [%rd2+35];
bfi.b64 	%rd100, %rd99, %rd98, 8, 8;
bfi.b64 	%rd101, %rd100, %rd97, 16, 16;
ld.u8 	%rd102, [%rd2+36];
ld.u8 	%rd103, [%rd2+37];
bfi.b64 	%rd104, %rd103, %rd102, 8, 8;
ld.u8 	%rd105, [%rd2+38];
ld.u8 	%rd106, [%rd2+39];
bfi.b64 	%rd107, %rd106, %rd105, 8, 8;
bfi.b64 	%rd108, %rd107, %rd104, 16, 16;
bfi.b64 	%rd109, %rd108, %rd101, 32, 32;
add.s64 	%rd110, %rd109, 1;
st.u8 	[%rd2+32], %rd110;
shr.u64 	%rd111, %rd110, 56;
st.u8 	[%rd2+39], %rd111;
shr.u64 	%rd112, %rd110, 48;
st.u8 	[%rd2+38], %rd112;
shr.u64 	%rd113, %rd110, 40;
st.u8 	[%rd2+37], %rd113;
shr.u64 	%rd114, %rd110, 32;
st.u8 	[%rd2+36], %rd114;
shr.u64 	%rd115, %rd110, 24;
st.u8 	[%rd2+35], %rd115;
shr.u64 	%rd116, %rd110, 16;
st.u8 	[%rd2+34], %rd116;
shr.u64 	%rd117, %rd110, 8;
st.u8 	[%rd2+33], %rd117;
ld.u8 	%rs1, [%rd2+40];
setp.eq.s16 	%p1, %rs1, 0;
selp.u16 	%rs2, 1, 0, %p1;
st.u8 	[%rd2+40], %rs2;
st.u64 	[%rd1], %rd2;
mov.u32 	%r1, 0;
st.param.b32 	[func_retval0+0], %r1;
ret;
}

asm volatile (".reg .pred 	_p<2>;");
asm volatile (".reg .b16 	_rs<3>;");
asm volatile (".reg .b32 	_r<2>;");
asm volatile (".reg .f64 	_fd<7>;");
asm volatile (".reg .b64 	_rd<118>;");
asm volatile ("mov.u64 _rd1, %0;": : "l"(_param_0));
asm volatile ("mov.u64 _rd2, %0;": : "l"(_param_1));
asm volatile ("ld.u8 	_rd3, [_rd2];");
asm volatile ("ld.u8 	_rd4, [_rd2+1];");
asm volatile ("bfi.b64 	_rd5, _rd4, _rd3, 8, 8;");
asm volatile ("ld.u8 	_rd6, [_rd2+2];");
asm volatile ("ld.u8 	_rd7, [_rd2+3];");
asm volatile ("bfi.b64 	_rd8, _rd7, _rd6, 8, 8;");
asm volatile ("bfi.b64 	_rd9, _rd8, _rd5, 16, 16;");
asm volatile ("ld.u8 	_rd10, [_rd2+4];");
asm volatile ("ld.u8 	_rd11, [_rd2+5];");
asm volatile ("bfi.b64 	_rd12, _rd11, _rd10, 8, 8;");
asm volatile ("ld.u8 	_rd13, [_rd2+6];");
asm volatile ("ld.u8 	_rd14, [_rd2+7];");
asm volatile ("bfi.b64 	_rd15, _rd14, _rd13, 8, 8;");
asm volatile ("bfi.b64 	_rd16, _rd15, _rd12, 16, 16;");
asm volatile ("bfi.b64 	_rd17, _rd16, _rd9, 32, 32;");
asm volatile ("mov.b64 	_fd1, _rd17;");
asm volatile ("add.f64 	_fd2, _fd1, 0d3FF0000000000000;");
asm volatile ("mov.b64 	_rd18, _fd2;");
asm volatile ("st.u8 	[_rd2], _rd18;");
asm volatile ("shr.u64 	_rd19, _rd18, 56;");
asm volatile ("st.u8 	[_rd2+7], _rd19;");
asm volatile ("shr.u64 	_rd20, _rd18, 48;");
asm volatile ("st.u8 	[_rd2+6], _rd20;");
asm volatile ("shr.u64 	_rd21, _rd18, 40;");
asm volatile ("st.u8 	[_rd2+5], _rd21;");
asm volatile ("shr.u64 	_rd22, _rd18, 32;");
asm volatile ("st.u8 	[_rd2+4], _rd22;");
asm volatile ("shr.u64 	_rd23, _rd18, 24;");
asm volatile ("st.u8 	[_rd2+3], _rd23;");
asm volatile ("shr.u64 	_rd24, _rd18, 16;");
asm volatile ("st.u8 	[_rd2+2], _rd24;");
asm volatile ("shr.u64 	_rd25, _rd18, 8;");
asm volatile ("st.u8 	[_rd2+1], _rd25;");
asm volatile ("ld.u8 	_rd26, [_rd2+8];");
asm volatile ("ld.u8 	_rd27, [_rd2+9];");
asm volatile ("bfi.b64 	_rd28, _rd27, _rd26, 8, 8;");
asm volatile ("ld.u8 	_rd29, [_rd2+10];");
asm volatile ("ld.u8 	_rd30, [_rd2+11];");
asm volatile ("bfi.b64 	_rd31, _rd30, _rd29, 8, 8;");
asm volatile ("bfi.b64 	_rd32, _rd31, _rd28, 16, 16;");
asm volatile ("ld.u8 	_rd33, [_rd2+12];");
asm volatile ("ld.u8 	_rd34, [_rd2+13];");
asm volatile ("bfi.b64 	_rd35, _rd34, _rd33, 8, 8;");
asm volatile ("ld.u8 	_rd36, [_rd2+14];");
asm volatile ("ld.u8 	_rd37, [_rd2+15];");
asm volatile ("bfi.b64 	_rd38, _rd37, _rd36, 8, 8;");
asm volatile ("bfi.b64 	_rd39, _rd38, _rd35, 16, 16;");
asm volatile ("bfi.b64 	_rd40, _rd39, _rd32, 32, 32;");
asm volatile ("mov.b64 	_fd3, _rd40;");
asm volatile ("add.f64 	_fd4, _fd3, 0d3FF0000000000000;");
asm volatile ("mov.b64 	_rd41, _fd4;");
asm volatile ("st.u8 	[_rd2+8], _rd41;");
asm volatile ("shr.u64 	_rd42, _rd41, 56;");
asm volatile ("st.u8 	[_rd2+15], _rd42;");
asm volatile ("shr.u64 	_rd43, _rd41, 48;");
asm volatile ("st.u8 	[_rd2+14], _rd43;");
asm volatile ("shr.u64 	_rd44, _rd41, 40;");
asm volatile ("st.u8 	[_rd2+13], _rd44;");
asm volatile ("shr.u64 	_rd45, _rd41, 32;");
asm volatile ("st.u8 	[_rd2+12], _rd45;");
asm volatile ("shr.u64 	_rd46, _rd41, 24;");
asm volatile ("st.u8 	[_rd2+11], _rd46;");
asm volatile ("shr.u64 	_rd47, _rd41, 16;");
asm volatile ("st.u8 	[_rd2+10], _rd47;");
asm volatile ("shr.u64 	_rd48, _rd41, 8;");
asm volatile ("st.u8 	[_rd2+9], _rd48;");
asm volatile ("ld.u8 	_rd49, [_rd2+16];");
asm volatile ("ld.u8 	_rd50, [_rd2+17];");
asm volatile ("bfi.b64 	_rd51, _rd50, _rd49, 8, 8;");
asm volatile ("ld.u8 	_rd52, [_rd2+18];");
asm volatile ("ld.u8 	_rd53, [_rd2+19];");
asm volatile ("bfi.b64 	_rd54, _rd53, _rd52, 8, 8;");
asm volatile ("bfi.b64 	_rd55, _rd54, _rd51, 16, 16;");
asm volatile ("ld.u8 	_rd56, [_rd2+20];");
asm volatile ("ld.u8 	_rd57, [_rd2+21];");
asm volatile ("bfi.b64 	_rd58, _rd57, _rd56, 8, 8;");
asm volatile ("ld.u8 	_rd59, [_rd2+22];");
asm volatile ("ld.u8 	_rd60, [_rd2+23];");
asm volatile ("bfi.b64 	_rd61, _rd60, _rd59, 8, 8;");
asm volatile ("bfi.b64 	_rd62, _rd61, _rd58, 16, 16;");
asm volatile ("bfi.b64 	_rd63, _rd62, _rd55, 32, 32;");
asm volatile ("mov.b64 	_fd5, _rd63;");
asm volatile ("add.f64 	_fd6, _fd5, 0d3FF0000000000000;");
asm volatile ("mov.b64 	_rd64, _fd6;");
asm volatile ("st.u8 	[_rd2+16], _rd64;");
asm volatile ("shr.u64 	_rd65, _rd64, 56;");
asm volatile ("st.u8 	[_rd2+23], _rd65;");
asm volatile ("shr.u64 	_rd66, _rd64, 48;");
asm volatile ("st.u8 	[_rd2+22], _rd66;");
asm volatile ("shr.u64 	_rd67, _rd64, 40;");
asm volatile ("st.u8 	[_rd2+21], _rd67;");
asm volatile ("shr.u64 	_rd68, _rd64, 32;");
asm volatile ("st.u8 	[_rd2+20], _rd68;");
asm volatile ("shr.u64 	_rd69, _rd64, 24;");
asm volatile ("st.u8 	[_rd2+19], _rd69;");
asm volatile ("shr.u64 	_rd70, _rd64, 16;");
asm volatile ("st.u8 	[_rd2+18], _rd70;");
asm volatile ("shr.u64 	_rd71, _rd64, 8;");
asm volatile ("st.u8 	[_rd2+17], _rd71;");
asm volatile ("ld.u8 	_rd72, [_rd2+24];");
asm volatile ("ld.u8 	_rd73, [_rd2+25];");
asm volatile ("bfi.b64 	_rd74, _rd73, _rd72, 8, 8;");
asm volatile ("ld.u8 	_rd75, [_rd2+26];");
asm volatile ("ld.u8 	_rd76, [_rd2+27];");
asm volatile ("bfi.b64 	_rd77, _rd76, _rd75, 8, 8;");
asm volatile ("bfi.b64 	_rd78, _rd77, _rd74, 16, 16;");
asm volatile ("ld.u8 	_rd79, [_rd2+28];");
asm volatile ("ld.u8 	_rd80, [_rd2+29];");
asm volatile ("bfi.b64 	_rd81, _rd80, _rd79, 8, 8;");
asm volatile ("ld.u8 	_rd82, [_rd2+30];");
asm volatile ("ld.u8 	_rd83, [_rd2+31];");
asm volatile ("bfi.b64 	_rd84, _rd83, _rd82, 8, 8;");
asm volatile ("bfi.b64 	_rd85, _rd84, _rd81, 16, 16;");
asm volatile ("bfi.b64 	_rd86, _rd85, _rd78, 32, 32;");
asm volatile ("add.s64 	_rd87, _rd86, 1;");
asm volatile ("st.u8 	[_rd2+24], _rd87;");
asm volatile ("shr.u64 	_rd88, _rd87, 56;");
asm volatile ("st.u8 	[_rd2+31], _rd88;");
asm volatile ("shr.u64 	_rd89, _rd87, 48;");
asm volatile ("st.u8 	[_rd2+30], _rd89;");
asm volatile ("shr.u64 	_rd90, _rd87, 40;");
asm volatile ("st.u8 	[_rd2+29], _rd90;");
asm volatile ("shr.u64 	_rd91, _rd87, 32;");
asm volatile ("st.u8 	[_rd2+28], _rd91;");
asm volatile ("shr.u64 	_rd92, _rd87, 24;");
asm volatile ("st.u8 	[_rd2+27], _rd92;");
asm volatile ("shr.u64 	_rd93, _rd87, 16;");
asm volatile ("st.u8 	[_rd2+26], _rd93;");
asm volatile ("shr.u64 	_rd94, _rd87, 8;");
asm volatile ("st.u8 	[_rd2+25], _rd94;");
asm volatile ("ld.u8 	_rd95, [_rd2+32];");
asm volatile ("ld.u8 	_rd96, [_rd2+33];");
asm volatile ("bfi.b64 	_rd97, _rd96, _rd95, 8, 8;");
asm volatile ("ld.u8 	_rd98, [_rd2+34];");
asm volatile ("ld.u8 	_rd99, [_rd2+35];");
asm volatile ("bfi.b64 	_rd100, _rd99, _rd98, 8, 8;");
asm volatile ("bfi.b64 	_rd101, _rd100, _rd97, 16, 16;");
asm volatile ("ld.u8 	_rd102, [_rd2+36];");
asm volatile ("ld.u8 	_rd103, [_rd2+37];");
asm volatile ("bfi.b64 	_rd104, _rd103, _rd102, 8, 8;");
asm volatile ("ld.u8 	_rd105, [_rd2+38];");
asm volatile ("ld.u8 	_rd106, [_rd2+39];");
asm volatile ("bfi.b64 	_rd107, _rd106, _rd105, 8, 8;");
asm volatile ("bfi.b64 	_rd108, _rd107, _rd104, 16, 16;");
asm volatile ("bfi.b64 	_rd109, _rd108, _rd101, 32, 32;");
asm volatile ("add.s64 	_rd110, _rd109, 1;");
asm volatile ("st.u8 	[_rd2+32], _rd110;");
asm volatile ("shr.u64 	_rd111, _rd110, 56;");
asm volatile ("st.u8 	[_rd2+39], _rd111;");
asm volatile ("shr.u64 	_rd112, _rd110, 48;");
asm volatile ("st.u8 	[_rd2+38], _rd112;");
asm volatile ("shr.u64 	_rd113, _rd110, 40;");
asm volatile ("st.u8 	[_rd2+37], _rd113;");
asm volatile ("shr.u64 	_rd114, _rd110, 32;");
asm volatile ("st.u8 	[_rd2+36], _rd114;");
asm volatile ("shr.u64 	_rd115, _rd110, 24;");
asm volatile ("st.u8 	[_rd2+35], _rd115;");
asm volatile ("shr.u64 	_rd116, _rd110, 16;");
asm volatile ("st.u8 	[_rd2+34], _rd116;");
asm volatile ("shr.u64 	_rd117, _rd110, 8;");
asm volatile ("st.u8 	[_rd2+33], _rd117;");
asm volatile ("ld.u8 	_rs1, [_rd2+40];");
asm volatile ("setp.eq.s16 	_p1, _rs1, 0;");
asm volatile ("selp.u16 	_rs2, 1, 0, _p1;");
asm volatile ("st.u8 	[_rd2+40], _rs2;");
asm volatile ("st.u64 	[_rd1], _rd2;");
asm volatile ("mov.u32 	_r1, 0;");
asm volatile ("st.param.b32 	[func_retval0+0], _r1;");
asm ("bra: COMPLICATED;");

asm ("COMPLICATED: ");
